{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 1193514 terminos con sus vectores de embedding.\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import numpy as np\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "sns.set_context(context=\"talk\")\n",
    "\n",
    "import os, re, sys\n",
    "from keras.layers import CuDNNLSTM,CuDNNGRU\n",
    "\n",
    "import pandas as pd\n",
    "import ast, time\n",
    "import matplotlib.pyplot as plt\n",
    "#conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import re, io, nltk, torch \n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm, trange\n",
    "from gensim import utils, matutils  # utility fnc for pickling, common scipy operations etc\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from six import string_types, integer_types\n",
    "from six.moves import zip, range\n",
    "from scipy import stats\n",
    "from gensim.utils import deprecated\n",
    "from numpy import dot, float32 as REAL, memmap as np_memmap, \\\n",
    "    double, array, zeros, vstack, sqrt, newaxis, integer, \\\n",
    "    ndarray, sum as np_sum, prod, argmax\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from numpy.random import binomial\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import Callback,ModelCheckpoint, ReduceLROnPlateau    \n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, Input\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM,GRU,Dense\n",
    "from keras.utils import Sequence,to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer_sn = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stoplist = stopwords.words(\"english\")\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def cleaner(post): \n",
    "    #re.sub(r'([a-z])\\1+', r'\\1', \"user i think that ' s all you loooooove \")\n",
    "    s= re.sub(r\"http\\S+\", \" <html> \", post)   ##########elimina http    \n",
    "    s= re.sub(r'#\\w+ ?', \" <hashtag> \", s) ########3\n",
    "    s= re.sub(r'@\\w+ ?', \" <user> \", s)##############    \n",
    "    s= s.lower()\n",
    "    s=emoji_pattern.sub(r'', s)\n",
    "    s=re.sub(r\"'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"\\b'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"“\\b\", \" “ \", s)\n",
    "    #“\n",
    "    s=re.sub(r\"\\b’\", \" ’ \", s)\n",
    "    s=re.sub(r\"‘\\b\", \" ‘ \", s)\n",
    "    s=re.sub(r\"\\b’\\b\", \" ‘ \", s)\n",
    "    s = re.sub(r\"-\", \" - \", s)\n",
    "    s = re.sub(r\"\\(\", \" \", s)\n",
    "    s = re.sub(r\"\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)    \n",
    "    s = re.sub(r\"\\/\", \" \", s)\n",
    "    s = re.sub(r\"' \", \" ' \", s)\n",
    "    s = re.sub(r\" '\", \" ' \", s)\n",
    "    s = re.sub(r\"\\!\", \" ! \", s)\n",
    "    s=re.sub(\"[\\.]+\", \" . \", s)\n",
    "    s=re.sub(\"[\\,]+\", \" , \", s)\n",
    "    s=re.sub(\"[\\;]+\", \" ; \", s)\n",
    "    s=re.sub(\"[\\:]+\", \" : \", s)\n",
    "    s=re.sub('[\\\"]+', ' \" ', s)\n",
    "    s=re.sub(r'\\b[0-9]\\b', \" <number> \",  s)\n",
    "    s=re.sub(r'\\b[0-9]*[0-9]\\b', \" <number> \",  s)    \n",
    "    s=re.sub(r'\\b”', ' \" ', s)\n",
    "    sl= list(s.split())\n",
    "    sl_2=[]\n",
    "    for wd in sl:\n",
    "        try: \n",
    "            q=Word2Index_valid[wd]\n",
    "            sl_2.append(wd)\n",
    "        except:\n",
    "            try: \n",
    "                if stemmer.stem(wd) in Word2Index_valid.keys():\n",
    "                    sl_2.append(stemmer.stem(wd))\n",
    "                elif lemmatizer.lemmatize(wd) in Word2Index_valid.keys():\n",
    "                    sl_2.append(lemmatizer.lemmatize(wd))\n",
    "                else:\n",
    "                    sl_2.append(wd)\n",
    "            except:\n",
    "                sl_2.append(wd)\n",
    "    sl=sl_2\n",
    "    s=' '.join([word for word in sl])# if word not in stoplist])\n",
    "    return s, sl\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(\"../../Datasets/glove.twitter.27B.200d.txt\"))#'glove.twitter.27B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Se encontraron %s terminos con sus vectores de embedding.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7239"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_cl=dict()\n",
    "dict_cl[0]='support'\n",
    "dict_cl[1]='denying'\n",
    "dict_cl[2]='questioning'\n",
    "dict_cl[3]='commenting'\n",
    "\n",
    "#sdqc\n",
    "d_lab=dict()\n",
    "d_lab[\"support\"]=0\n",
    "d_lab[\"denying\"]=1\n",
    "d_lab[\"questioning\"]=2\n",
    "d_lab[\"commenting\"]=3\n",
    "\n",
    "\n",
    "path=\"../../Datasets/CSV_Stance/\"\n",
    "train =pd.read_csv(path + \"train_semeval_raw.csv\")\n",
    "val=pd.read_csv(path + \"dev_semeval_raw.csv\")\n",
    "test=pd.read_csv(path + \"test_semeval_raw.csv\")\n",
    "\n",
    "word_index=dict()\n",
    "j=1\n",
    "for frase in train[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "\n",
    "for frase in val[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "            \n",
    "for frase in test[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "            \n",
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found=[]\n",
    "pos_tag_nf=[]\n",
    "Word2Index={}\n",
    "Word2Index_valid={}\n",
    "idx_2_token={}\n",
    "idx_2_token_valid={}\n",
    "\n",
    "g_dim=200\n",
    "emb_matrix = np.zeros((len(word_index.keys())+1, g_dim))  \n",
    "\n",
    "k=1\n",
    "for word, i in word_index.items():\n",
    "    vector = embeddings_index.get(word)\n",
    "    if vector is not None:\n",
    "        # words sin match en Glove, serán vectores de ceros.\n",
    "        emb_matrix[i] = vector\n",
    "        Word2Index[word]=i\n",
    "        Word2Index_valid[word]=k\n",
    "        idx_2_token_valid[k]=word\n",
    "        k+=1\n",
    "        idx_2_token[i]=word\n",
    "        \n",
    "    else:\n",
    "        Word2Index[word]=i\n",
    "        idx_2_token[i]=word\n",
    "        not_found.append(word)\n",
    "        pos_tag_nf.append(nltk.pos_tag([word])[0][-1])\n",
    "\n",
    "        \n",
    "glove_matrix = np.zeros((k, g_dim))\n",
    "\n",
    "for word, i in Word2Index_valid.items():\n",
    "    vector = embeddings_index.get(word)    \n",
    "    glove_matrix[i] = vector\n",
    "    \n",
    "glove_matrix.shape\n",
    "\n",
    "M_GLOVE_space= glove_matrix\n",
    "\n",
    "def my_tokenizer(lista):\n",
    "    encoded_sent=[]\n",
    "    to_return_ide=[]\n",
    "    for wd in lista:\n",
    "        try:               \n",
    "            to_return_ide.append(Word2Index_valid[wd])\n",
    "            encoded_sent.append(wd)\n",
    "        except: \n",
    "            continue   \n",
    "           \n",
    "    return to_return_ide, encoded_sent\n",
    "\n",
    "def predict_data(trained, x_train, x_val, x_test, etiq, etiq_v, etiq_t, name_model):\n",
    "    etiq = etiq.astype(\"int\")\n",
    "    trainPredict = trained.predict(x_train, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq, trainPredict)  \n",
    "    f1=f1_score(etiq, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Train\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Train\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Train\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_val, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq_v, trainPredict)  \n",
    "    f1=f1_score(etiq_v, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq_v, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq_v, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Val\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Val\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Val\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_test, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc_t= accuracy_score(etiq_t, trainPredict)  \n",
    "    f1_t=f1_score(etiq_t, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma_t=f1_score(etiq_t, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    matriz_t=normalize(confusion_matrix(etiq_t, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Test\", name_model, \":\",acc_t)  \n",
    "    print (\"F1-score None sobre Test\", name_model, \":\",f1_t)\n",
    "    print (\"F1-score macro sobre Test\", name_model, \":\",f1_ma_t)\n",
    "    \n",
    "    return f1_ma_t, f1_t, acc_t, matriz_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo por stance val\n",
      " Counter({'commenting': 173, 'support': 69, 'questioning': 28, 'denying': 11})\n",
      "Conteo por stance test\n",
      " Counter({'commenting': 778, 'questioning': 106, 'denying': 69, 'support': 68})\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 37\n",
    "\n",
    "print (\"Conteo por stance val\\n\", Counter(val['Label']))\n",
    "print (\"Conteo por stance test\\n\", Counter(test['Label']))\n",
    "\n",
    "ids_train=train['Tw_id']\n",
    "tws_train=train['Tweet']\n",
    "labels_train=[d_lab[lb] for lb in train['Label']]\n",
    "\n",
    "etiq=labels_train.copy()\n",
    "etiq=np.asarray(etiq)\n",
    "sentences = tws_train\n",
    "n_labels = np.array(etiq)\n",
    "y_train=to_categorical(n_labels,num_classes=4)\n",
    "\n",
    "\n",
    "input_ids=[]\n",
    "for sent in sentences:\n",
    "    encoded_sent, sent_valid = my_tokenizer(cleaner(sent)[1])  \n",
    "    input_ids.append(encoded_sent) \n",
    "    \n",
    "input_ids = pad_sequences(input_ids,maxlen=MAX_LEN,dtype=\"long\",value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    \n",
    "    \n",
    "ids_val=val['Tw_id']\n",
    "tws_val=val['Tweet']\n",
    "labels_val=[d_lab[lb] for lb in val['Label']]\n",
    "\n",
    "etiq_v=labels_val.copy()\n",
    "etiq_v=np.asarray(etiq_v)\n",
    "sentences_val = tws_val\n",
    "n_labels_val = np.array(etiq_v)\n",
    "y_val=to_categorical(n_labels_val,num_classes=4)\n",
    "\n",
    "input_ids_val=[]\n",
    "for sent in sentences_val:\n",
    "    encoded_sent, sent_valid =my_tokenizer(cleaner(sent)[1])  \n",
    "    input_ids_val.append(encoded_sent)\n",
    "\n",
    "ids_test=test['Tw_id']\n",
    "tws_test=test['Tweet']\n",
    "labels_test=[d_lab[lb] for lb in test['Label']]\n",
    "\n",
    "etiq_t=labels_test.copy()\n",
    "etiq_t=np.asarray(etiq_t)\n",
    "sentences_test = tws_test\n",
    "n_labels_test = np.array(etiq_t)\n",
    "y_test=to_categorical(n_labels_test,num_classes=4)\n",
    "\n",
    "input_ids_test=[]\n",
    "for sent in sentences_test:\n",
    "    encoded_sent, sent_valid = my_tokenizer(cleaner(sent)[1])\n",
    "    input_ids_test.append(encoded_sent)\n",
    "    \n",
    "input_ids_val = pad_sequences(input_ids_val,maxlen=MAX_LEN,dtype=\"long\",value=0, truncating=\"post\", padding=\"post\")\n",
    "input_ids_test = pad_sequences(input_ids_test,maxlen=MAX_LEN,dtype=\"long\",value=0, truncating=\"post\", padding=\"post\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=np.asarray(input_ids).shape\n",
    "x_train=np.zeros((shape[0], g_dim))\n",
    "i=0\n",
    "for in_id in input_ids:\n",
    "    vector=np.zeros(g_dim)\n",
    "    n = np.sum(in_id != 0)\n",
    "    if n==0:\n",
    "        x_train[i]=vector\n",
    "        i+=1\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        for idv in in_id:\n",
    "            if idv!=0:\n",
    "                vector+=M_GLOVE_space[idv]\n",
    "        x_train[i]=(vector/n)\n",
    "        i+=1   \n",
    "    \n",
    "    \n",
    "shape_val=np.asarray(input_ids_val).shape\n",
    "x_val=np.zeros((shape_val[0], g_dim))\n",
    "i=0\n",
    "for in_id in input_ids_val:\n",
    "    vector=np.zeros(g_dim)\n",
    "    n = np.sum(in_id != 0)\n",
    "    for idv in in_id:\n",
    "        if idv!=0:\n",
    "            vector+=M_GLOVE_space[idv]\n",
    "    x_val[i]=(vector/n)\n",
    "    i+=1  \n",
    "    \n",
    "    \n",
    "shape_test=np.asarray(input_ids_test).shape\n",
    "x_test=np.zeros((shape_test[0], g_dim))\n",
    "i=0\n",
    "for in_id in input_ids_test:\n",
    "    vector=np.zeros(g_dim)\n",
    "    n = np.sum(in_id != 0)\n",
    "    for idv in in_id:\n",
    "        if idv!=0:\n",
    "            vector+=M_GLOVE_space[idv]\n",
    "    x_test[i]=(vector/n)\n",
    "    i+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector nulo\n"
     ]
    }
   ],
   "source": [
    "emb=200 \n",
    "\n",
    "x_train=np.zeros((shape[0], emb))\n",
    "for i,in_id in enumerate(input_ids):\n",
    "    vector=np.zeros(emb)\n",
    "    n = np.sum(in_id != 0)\n",
    "    if n==0:\n",
    "        x_train[i]=vector \n",
    "        print (\"Vector nulo\")\n",
    "    else:\n",
    "        for idv in in_id:\n",
    "            if idv!=0:\n",
    "                vector+=M_GLOVE_space[idv]\n",
    "        x_train[i]=(vector/n)   \n",
    "    \n",
    "x_val=np.zeros((shape_val[0], emb))\n",
    "for i,in_id in enumerate(input_ids_val):\n",
    "    vector=np.zeros(emb)\n",
    "    n = np.sum(in_id != 0)\n",
    "    if n==0:\n",
    "        x_train[i]=vector    \n",
    "        print (\"Vector nulo - val\")\n",
    "    else:\n",
    "        for idv in in_id:\n",
    "            if idv!=0:\n",
    "                vector+=M_GLOVE_space[idv]\n",
    "        x_train[i]=(vector/n)   \n",
    "    \n",
    "x_test=np.zeros((shape_test[0], emb))\n",
    "for i,in_id in enumerate(input_ids_test):\n",
    "    vector=np.zeros(emb)\n",
    "    n = np.sum(in_id != 0)\n",
    "    if n==0:\n",
    "        print (\"Vector nulo - test\")\n",
    "        x_train[i]=vector       \n",
    "    else:\n",
    "        for idv in in_id:\n",
    "            if idv!=0:\n",
    "                vector+=M_GLOVE_space[idv]\n",
    "        x_train[i]=(vector/n)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13629156,  0.18461726, -0.11320943, ...,  0.14865043,\n",
       "        -0.10502915,  0.29948328],\n",
       "       [ 0.16448128,  0.31063627,  0.01793911, ...,  0.06655957,\n",
       "        -0.111752  ,  0.18355334],\n",
       "       [ 0.13947495,  0.14317134, -0.04588211, ..., -0.04316833,\n",
       "        -0.13885662,  0.2402154 ],\n",
       "       ...,\n",
       "       [ 0.06276683,  0.1871345 , -0.06782619, ...,  0.04203048,\n",
       "        -0.12054235,  0.10020556],\n",
       "       [ 0.223201  ,  0.19933249,  0.152025  , ..., -0.635405  ,\n",
       "        -0.43630251,  0.74134497],\n",
       "       [ 0.33292   ,  0.36736666,  0.03645428, ..., -0.26799167,\n",
       "        -0.38191934,  0.59850499]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12260148,  0.06784743,  0.03343611, ..., -0.01884784,\n",
       "         0.02212088,  0.14888648],\n",
       "       [-0.03001559,  0.09751672,  0.01159959, ..., -0.00525489,\n",
       "         0.09056827,  0.17004077],\n",
       "       [ 0.11304514,  0.11763916, -0.07437836, ...,  0.10727124,\n",
       "        -0.05845142,  0.16853503],\n",
       "       ...,\n",
       "       [ 0.12979192,  0.15855721,  0.00943488, ..., -0.04692357,\n",
       "        -0.06332121,  0.19290788],\n",
       "       [-0.05821643,  0.14426113,  0.04274825, ...,  0.101905  ,\n",
       "         0.00380737, -0.07583332],\n",
       "       [ 0.13209359,  0.13499256,  0.03190052, ..., -0.01944156,\n",
       "        -0.28839688, -0.01245812]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12260148,  0.06784743,  0.03343611, ..., -0.01884784,\n",
       "         0.02212088,  0.14888648],\n",
       "       [-0.03001559,  0.09751672,  0.01159959, ..., -0.00525489,\n",
       "         0.09056827,  0.17004077],\n",
       "       [ 0.11304514,  0.11763916, -0.07437836, ...,  0.10727124,\n",
       "        -0.05845142,  0.16853503],\n",
       "       ...,\n",
       "       [ 0.12979192,  0.15855721,  0.00943488, ..., -0.04692357,\n",
       "        -0.06332121,  0.19290788],\n",
       "       [-0.05821643,  0.14426113,  0.04274825, ...,  0.101905  ,\n",
       "         0.00380737, -0.07583332],\n",
       "       [ 0.13209359,  0.13499256,  0.03190052, ..., -0.01944156,\n",
       "        -0.28839688, -0.01245812]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13629156,  0.18461726, -0.11320943, ...,  0.14865043,\n",
       "        -0.10502915,  0.29948328],\n",
       "       [ 0.16448128,  0.31063627,  0.01793911, ...,  0.06655957,\n",
       "        -0.111752  ,  0.18355334],\n",
       "       [ 0.13947495,  0.14317134, -0.04588211, ..., -0.04316833,\n",
       "        -0.13885662,  0.2402154 ],\n",
       "       ...,\n",
       "       [ 0.06276683,  0.1871345 , -0.06782619, ...,  0.04203048,\n",
       "        -0.12054235,  0.10020556],\n",
       "       [ 0.223201  ,  0.19933249,  0.152025  , ..., -0.635405  ,\n",
       "        -0.43630251,  0.74134497],\n",
       "       [ 0.33292   ,  0.36736666,  0.03645428, ..., -0.26799167,\n",
       "        -0.38191934,  0.59850499]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12260148,  0.06784743,  0.03343611, ..., -0.01884784,\n",
       "         0.02212088,  0.14888648],\n",
       "       [-0.03001559,  0.09751672,  0.01159959, ..., -0.00525489,\n",
       "         0.09056827,  0.17004077],\n",
       "       [ 0.11304514,  0.11763916, -0.07437836, ...,  0.10727124,\n",
       "        -0.05845142,  0.16853503],\n",
       "       ...,\n",
       "       [ 0.12979192,  0.15855721,  0.00943488, ..., -0.04692357,\n",
       "        -0.06332121,  0.19290788],\n",
       "       [-0.05821643,  0.14426113,  0.04274825, ...,  0.101905  ,\n",
       "         0.00380737, -0.07583332],\n",
       "       [ 0.13209359,  0.13499256,  0.03190052, ..., -0.01944156,\n",
       "        -0.28839688, -0.01245812]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando SMOTE  1 \n",
      "\n",
      "(10936,)\n",
      "class_weights Training set [1. 1. 1. 1.]\n",
      "\n",
      "Accuracy sobre Train cnn1 : 0.6955010972933431\n",
      "F1-score None sobre Train cnn1 : [0.62018592 0.73340061 0.80457344 0.62585642]\n",
      "F1-score macro sobre Train cnn1 : 0.6960040966800356\n",
      "\n",
      "Accuracy sobre Val cnn1 : 0.6156583629893239\n",
      "F1-score None sobre Val cnn1 : [0.         0.         0.         0.76211454]\n",
      "F1-score macro sobre Val cnn1 : 0.1905286343612335\n",
      "\n",
      "Accuracy sobre Test cnn1 : 0.761998041136141\n",
      "F1-score None sobre Test cnn1 : [0.         0.         0.         0.86492496]\n",
      "F1-score macro sobre Test cnn1 : 0.2162312395775431\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f59d7bd87124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcnn2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_cnn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mf1_ma_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatriz_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_smote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metiq_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metiq_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cnn2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mfs_macro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cnn2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tesis/Text_Gen/SMOTE/models.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_val, y_val, class_weights, e, BS, verb, focal)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fs_macro={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "for i in range(1,2):    \n",
    "    print (\"\\nEntrenando SMOTE \", i,\"\\n\")\n",
    "    \n",
    "    sm = SMOTE(random_state=27, n_jobs=-1, sampling_strategy='not majority')\n",
    "    x_train_smote, y_train_smote = sm.fit_sample(np.asarray(x_train, dtype='float32'), np.asarray(n_labels, dtype='float32'))\n",
    "    print (y_train_smote.shape)\n",
    "\n",
    "    x_new_train=np.expand_dims(x_train_smote, axis=-1)\n",
    "    x_new_val=np.expand_dims(x_val, axis=-1)\n",
    "    x_new_test=np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "    y_train=to_categorical(y_train_smote,num_classes=4)\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_smote), y_train_smote)\n",
    "    print (\"class_weights Training set\", class_weights)\n",
    "\n",
    "    cnn1=create_cnn1(x_new_train.shape[1:])\n",
    "    trained, hist =train_model(cnn1, x_new_train, y_train, x_new_val, y_val, class_weights, e=8, BS=32, verb=0, focal=True)\n",
    "    f1_ma_t, f1_t, acc_t, matriz_t = predict_data(trained, x_new_train, x_new_val, x_new_test, y_train_smote, etiq_v, etiq_t, 'cnn1')\n",
    "    fs_macro['cnn1'].append(f1_ma_t)\n",
    "    fs_none['cnn1'].append(f1_t)\n",
    "    accs['cnn1'].append(acc_t)\n",
    "    confusions['cnn1'].append(matriz_t)\n",
    "    \n",
    "    \n",
    "    cnn2=create_cnn2(x_new_train.shape[1:])\n",
    "    trained, hist =train_model(cnn2, x_new_train, y_train, x_new_val, y_val, class_weights, e=8, BS=32, verb=0, focal=True)\n",
    "    f1_ma_t, f1_t, acc_t, matriz_t = predict_data(trained, x_new_train, x_new_val, x_new_test, y_train_smote, etiq_v, etiq_t, 'cnn2')\n",
    "    fs_macro['cnn2'].append(f1_ma_t)\n",
    "    fs_none['cnn2'].append(f1_t)\n",
    "    accs['cnn2'].append(acc_t)\n",
    "    confusions['cnn2'].append(matriz_t)\n",
    "\n",
    "    rnn1=create_complex_GRU_2(128,64,\"adam\",x_new_train.shape[1:])\n",
    "    trained, hist =train_model(rnn1, x_new_train, y_train, x_new_val, y_val, class_weights, e=4, BS=32, verb=0, focal=True)\n",
    "    f1_ma_t, f1_t, acc_t, matriz_t = predict_data(trained, x_new_train, x_new_val, x_new_test, y_train_smote, etiq_v, etiq_t, 'rnn1')\n",
    "    fs_macro['rnn1'].append(f1_ma_t)\n",
    "    fs_none['rnn1'].append(f1_t)\n",
    "    accs['rnn1'].append(acc_t)\n",
    "    confusions['rnn1'].append(matriz_t)\n",
    "\n",
    "    rnn2=create_complex_GRU_3(256,128,64,\"adam\",x_new_train.shape[1:])\n",
    "    trained, hist =train_model(rnn2, x_new_train, y_train, x_new_val, y_val, class_weights, e=8, BS=32, verb=0, focal=False)\n",
    "    f1_ma_t, f1_t, acc_t, matriz_t = predict_data(trained, x_new_train, x_new_val, x_new_test, y_train_smote, etiq_v, etiq_t, 'rnn2')\n",
    "    fs_macro['rnn2'].append(f1_ma_t)\n",
    "    fs_none['rnn2'].append(f1_t)\n",
    "    accs['rnn2'].append(acc_t)\n",
    "    confusions['rnn2'].append(matriz_t)\n",
    "\n",
    "    rnn3=create_complex_GRU_3(256,128,64,\"adam\",x_new_train.shape[1:])\n",
    "    trained, hist =train_model(rnn3, x_new_train, y_train, x_new_val, y_val, class_weights, e=4, BS=32, verb=0, focal=True)\n",
    "    f1_ma_t, f1_t, acc_t, matriz_t = predict_data(trained, x_new_train, x_new_val, x_new_test, y_train_smote, etiq_v, etiq_t, 'rnn3')\n",
    "    fs_macro['rnn3'].append(f1_ma_t)\n",
    "    fs_none['rnn3'].append(f1_t)\n",
    "    accs['rnn3'].append(acc_t)\n",
    "    confusions['rnn3'].append(matriz_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
